We all know Tony Stark’s artificial intelligence system Jarvis from the marvel movies, iron man. It is in charge of business, security, health, and general assistance for Stark. Jarvis is just a fictional system however there are real life examples of how we have used artificial intelligence to help modern day society.

Siri is a virtual assistant used on apple devices. Siri first appeared as an app for IOS in 2010 and then became fully integrated into the iPhone 4S in 2011. Its developers have programmed its voice recognition software to interpret commands and questions through a series of steps allowing people to interact with Siri in as human a way as possible. Siri is based on the fields of artificial intelligence and natural language processing and it consists of three components - a conversational interface, personal context awareness and service delegation. A conversational interface is any UI that mimics chatting with a real human. The idea here is that instead of communicating with a computer on its own inhuman terms, you interact with it on your terms. Context awareness is the ability of systems to respond to user requests based on information about their environment or the context of operations. Service delegation is pretty self-explanatory. It is where they take the command and turn it into a system friendly task which is then performed by the requested or correct service. All this makes for a very clever system. The algorithm is complex enough that it is capable of working around idioms, homophones, and other literary expressions to determine the context of the sentence. The general workings of word-for-word voice recognition has to be good in order to hear what you’re saying, but deciphering the meaning is all down to statistics and machine learning. Eventually, Siri can predict what you’re requesting based on keywords as well as general habits and language choices. It is designed to adapt to your individual preferences over time and personalise results. 

In 2016, Mark Zuckerberg built an artificially intelligent voice-controlled assistant for his home. Coded in python, php and objective c, he spent a total of 100 hours building the assistant. With a few hardware modifications, such as using a 1950s toaster rigged up to a connected switch, configuring a food dispenser and t-shirt cannon, he was ready to start controlling it. His first task was to use natural language processing to control it as if he was speaking to a human. This was a two-step process: first have it communicable via text and then later the ability to be voice controlled. It started simple by looking for keywords like kitchen, lights, on and off to determine what was being said and through machine learning, quickly learnt to recognise synonyms like family room and living room. Speech recognition was a key part of the project. Learning to recognise the difference between the family’s voices and which room the command was being requested from was one of the main things Zuckerberg wanted to implement. This way requesting music could be personalised and you wouldn’t get lights turned off in the wrong room. Through a system of positive and negative feedback, the AI soon learnt who was who and where they were. To control the assistant, Zuckerberg built two systems. A messenger bot and a custom app. Messenger has a simple framework for building bots and it automatically handles a lot of things for you. This way, Jarvis could be used when voice commands were not appropriate. Zuckerberg then made a custom speech recognition app so that it could be controlled via phone microphones. To have multi room voice bases, more phones with the app running were added. The main reason for this is so that Jarvis could be used outside of the house, unlike smart devices like Alexa. For voice assistants to become more commercially used, Zuckerberg commented that more devices needed to be connected and the industry needed to develop common APIs and standards for the devices to talk to each other before we get anywhere near to iron man’s Jarvis.

A team of researchers in China have invented a deep learning algorithm that can determine an individual’s risk of heart disease. Alopecia, yellow eyelids, and an opaque ring around the eye’s cornea are among several clues in a person’s face that they could be suffering from poor cardiovascular health. The algorithms are still in development, but experts say it has the potential to revolutionise medicine. Between 2017 and 2019, researchers photographed 5,796 patients who had gone to the hospital for heart imaging procedures. Four images were taken - one full-front, two side profiles and one looking down on top of the head. The researchers trained the deep learning algorithm to analyse the four images and assess each person’s risk of heart disease. The results were published in the European heart journal and saw that the algorithm correctly detected heart disease in 80% of the group. It was also capable of detecting patients without coronary artery disease 61% of the time. Any additional clinical information did not improve its performance, which means it can be easily used to predict potential heart disease on just facial imaging alone. However, there are always potential ethical challenges. Such technology may raise concerns about misuse of information through discriminatory purposes such as a rise in prices of insurance for those who have been flagged as a risk or the neglect of insurance all together. 

After researching all this, I decided I wanted to try and create my very own personal assistant. It doesn’t have any AI capabilities, but I programmed a few basic commands in python that it will listen for and respond to. The first task was to install a speech to text conversion library. I decided to use pyttsx3 mainly because of its offline abilities. After that was set up and the speaking function was written, I moved on to writing some of the content. I wanted there to be an introduction from my assistant every time it loaded. I installed the datetime module and set it so it would say good morning, afternoon, and evening before introducing itself. Next was setting it up so it would recognise my speech. I used Google’s speech recognition for this. Jarvis would listen for a response, process the command, and then act on what was spoken. I also put in a try/except box for when it was unable to recognise my voice. With the functions down, I moved onto the main commands. There are a few different commands that I programmed. For example, searching Wikipedia and it will summarize the page for you. Opening certain websites or applications like YouTube or word. I included some basic conversational greetings as well. Many of the smart device assistants have the option to tell you a joke and this is something I wanted to include. After some research I found the library, pyjokes. All you need is pyjokes.get_joke() and it will output a computer science-based joke for you. Next on my list was to have a calculate function. For this I used the wolfram alpha api. I sliced the query into just the calculation being asked and ran it through wolfram alpha. It then gave the answer back and I outputted it. Searching for things was also relatively simple. I used the web browser library to search for things, I also used this for when I didn’t have a programmed answer and it would search the web instead after I confirmed that’s what I wanted it to do. One of my favourite parts was the ability to lock my device. After hearing “lock window”, the program would run the lock workstation function and my device would lock itself. What I made was far behind any AI capabilities, but it was good to see how someone would go about creating a virtual system and how it isn’t as complicated as you think.

AI currently lacks what most 10 years olds have, ordinary common sense. In 1950, Alan Turning came up with the idea of a “child machine”, or seed AI, where instead of trying to produce a program to stimulate the adult mind, why not train an AI to stimulate a child’s mind? Seed AI would have to possess ‘recursive self-improvement’ capabilities where it would be able to iteratively improves itself by recursively rewriting its own source code without human intervention. A Seed AI could start off with a relatively low level of intelligence. However, if it is intelligent enough to rewrite its source code it will gradually improve until it becomes super intelligent. 

To summarise, AI is pretty incredible. It can do so many things. AI has begun to take over our lives, with smart devices to self-driving cars to AI doctors. The big question here is, will AI take over the world like it has done in the marvel cinematic universe and many other movies and tv shows like doctor who and Star Wars? As for now the answer is no. Nobody has created anything close to the intelligence needed for AI to be a major threat. So, for now, we are safe from AI world domination until a real-life Tony Stark is born into the world. Thank you for listening.
